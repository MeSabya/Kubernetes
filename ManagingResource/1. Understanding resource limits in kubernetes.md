# Understanding resource limits in kubernetes

Resource limits are the operating parameters that you provide to kubernetes that tell it two critical things about your workload: 

👉 what resources it requires to run properly; and 

👉 the maximum resources it is allowed to consume.

*The first is a critical input to the scheduler that enables it to choose the right node on which to run the pod. 
The second is important to the kubelet, the daemon on each node that is responsible for pod health.*

## Resource limits

- Resource limits are set on a per-container basis using the resources property of a containerSpec
- Each object specifies both **“limits” and “requests”** for the types of resources that can be controlled. Currently that means **cpu and memory**

```yaml
resources:
  requests:
    cpu: 50m
    memory: 50Mi
  limits:
    cpu: 100m
    memory: 100Mi
```
👆 Says : 

>in normal operation this container needs 5 percent of cpu time, and 50 mebibytes of ram (the request); the maximum it is allowed to use is 10 percent of cpu time and 100 mebibytes of ram (the limit).

** Requests are important at schedule time, and limits are important at run time. Although resource limits are set on each container, you can think of the limits for a pod as being the sum of the limits of all the containers in it**

### Memory limits
#### how limits are implemented in the system ?

kubernetes delegates to the container runtime (docker/containerd in this case), and the container runtime delegates to the linux kernel. 
```
resources:
  requests:
    memory: 50Mi
  limits:
    memory: 100Mi
```
👉 In order to control the amount of memory that a container process can access docker configures a property of a control group, or cgroup for short

>cgroup is a container for a set of related properties that control how the kernel runs a process. There are specific cgroups to control memory, cpu, devices, etc. Cgroups are hierarchical, meaning that each cgroup has a parent from which it inherits properties, all the way up to the root cgroup which is created at system start.

##### Inspecting Cgroups 

Cgroups are easy to inspect using the /proc and /sys pseudo file systems, so it’s a simple exercise to see how docker has configured the memory cgroup for our container. 

```bash
$ ps ax | grep /bin/sh
   9513 ?        Ss     0:00 /bin/sh -c while true; do sleep 2; done
$ sudo cat /proc/9513/cgroup
...
6:memory:/kubepods/burstable/podfbc202d3-da21-11e8-ab5e-42010a80014b/0a1b22ec1361a97c3511db37a4bae932d41b22264e5b97611748f8b662312574
```
As you can see here’s that hierarchy I mentioned above. 

first the path begins with the kubepods cgroup, so our process will inherit everything in that group, as well as stuff from the burstable group (where kubernetes places processes from pods in the burstable QOS class) and a group representing our pod that is mostly used for accounting. The last component of the path is the actual memory cgroup of our process

To see the details we have to append the path above to /sys/fs/cgroups/memory, which leads to:

```bash
$ ls -l /sys/fs/cgroup/memory/kubepods/burstable/podfbc202d3-da21-11e8-ab5e-42010a80014b/0a1b22ec1361a97c3511db37a4bae932d41b22264e5b97611748f8b662312574
...
-rw-r--r-- 1 root root 0 Oct 27 19:53 memory.limit_in_bytes
-rw-r--r-- 1 root root 0 Oct 27 19:53 memory.soft_limit_in_bytes
```

##### What will happen when you run a POD not setting a memory limit in kubernetes?

Setting a memory limit in kubernetes caused docker to create the container with HostConfig.Memory set to 0, which resulted in the container process being placed into a memory cgroup with the default “no limit” value for memory.limit_in_bytes
Now let’s create a pod with a memory limit of 100 mebibytes:

```yaml
kubectl run limit-test --image=busybox --limits "memory=100Mi" --command -- /bin/sh -c "while true; do sleep 2; done"
deployment.apps "limit-test" created
```

And again we can use kubectl to verify that the pod was created with our specified limit:

```yaml
$ kubectl get pods limit-test-5f5c7dc87d-8qtdx -o=jsonpath='{.spec.containers[0].resources}'
map[limits:map[memory:100Mi] requests:map[memory:100Mi]]
```
When you set a limit, but not a request, kubernetes defaults the request to the limit.

### CPU limits 

```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    lab: pod
  name: dateloop
spec:
  containers:
  - image: acs/dateloop
    name: dateloop
    resources:
      requests:
        memory: 10Mi
        cpu: 50m
      limits:
        memory: 20Mi
        cpu: 100m

```

>The unit suffix m stands for “thousandth of a core,” so this resources object specifies that the container process needs 50/1000 of a core (5%) and is allowed to use at most 100/1000 of a core (10%). Likewise 2000m would be two full cores, which can also be specified as 2 or 2.0

👉 *Let’s create a pod with just a request for cpu and see how this is configured at the docker and cgroup levels:*

```bash
$ docker ps | grep busy | cut -d' ' -f1
f2321226620e
$ docker inspect f2321226620e --format '{{.HostConfig.CpuShares}}'
51
```

Why 51, and not 50? The cpu control group and docker both divide a core into 1024 shares, whereas kubernetes divides it into 1000. 

***How does docker apply this request to the container process?***

In the same way that setting memory limits caused docker to configure the process’s memory cgroup, setting cpu limits causes it to configure the cpu,cpuacct cgroup.

```bash
docker inspect 85045448a04b --format '{{.HostConfig.CpuShares}} {{.HostConfig.CpuQuota}} {{.HostConfig.CpuPeriod}}'
51 10000 100000
```

👉The cpu request is stored in the HostConfig.CpuShares property as we saw above. 

👉It is represented by two values: HostConfig.CpuPeriod and HostConfig.CpuQuota. These docker container config properties map to two additional properties of the process’s cpu,cpuacct cgroup: cpu.cfs_period_us and cpu.cfs_quota_us.

##### But how do the values of these two properties derive from the 100m cpu limit setting in our pod, and how do they implement that limit?

>The bandwidth control system defines 
> **a period, is usually 1/10 of a second, or 100000 microseconds** and 
> ***a quota which represents the number of microseconds of CPU time a container can use per cpu_period.***

👉 *After a container uses its cpu_quota it is throttled for the remainder of the cpu_period.*

**In this example we asked for a cpu limit of 100m on our pod. That is 100/1000 of a core, or 10000 out of 100000 microseconds of cpu time. 
So our limit request translates to setting cpu.cfs_period_us=100000 and cpu.cfs_quota_us=10000 on the process’s cpu,cpuacct cgroup.**

The cfs in those names, by the way, stands for Completely Fair Scheduler, which is the default linux cpu scheduler.

##### What happens in case of Exceeding?
Exceeding a memory limit makes your container process a candidate for oom-killing, 
whereas your process basically can’t exceed the set cpu quota, and will never get evicted for trying to use more cpu time than allocated. 
The system enforces the quota at the scheduler so the process just gets throttled at the limit.

##### What is the Default Memory/CPU for a Pod?
Kubernetes doesn’t provide default resource limits out-of-the-box. This means that unless you explicitly define limits, your containers can consume unlimited CPU and memory.

### LimitRange
You can also define a default limit for pods that don’t specify their own limits. This is done by creating aLimitRange in the relevant namespace:

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: default-limit
spec:
  limits:
  - default:
      memory: 100Mi
      cpu: 100m
    defaultRequest:
      memory: 50Mi
      cpu: 50m
  - max:
      memory: 512Mi
      cpu: 500m
  - min:
      memory: 50Mi
      cpu: 50m
    type: Container
```
Pods deployed **after** this LimitRange, without their own CPU or memory limit, will have these limits applied to them automatically.
Containers that were created before the LimitRange will not be affected by it.

-  The default key under limits represents the default limits for each resource.
-  The defaultRequest key is for resource requests. 
-  The max and min keys are something a little different: basically if these are set a pod will not be admitted to the namespace if it sets a request or limit that violates these bounds.




 
























