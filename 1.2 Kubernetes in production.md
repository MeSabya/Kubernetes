# Kubernetes in production: Five challenges you’re likely to face and how to approach them

## #1 Making Kubernetes deployments work at scale on demand.

- You might have to configure a load balancer like HAProxy or NGINX, if you’re deploying k8s anywhere other than the Google Cloud Engine.
- You can not afford to skip specifications like resource or request limits.
- You must implement graceful pod termination to downscale safely.
- You must design your autoscaling in a manner that Horizontal Pod Autoscaler (HPA) and Vertical Pod Autoscaler (VPA) don’t apply together.

## #2 Ensuring reliability

- You need multi-master setups for high-availability. And then build redundancies at application and infra-level for good measure.
- You must plan zero-downtime environment upgrades. You must also be patching applications and upgrading Kubernetes to its latest version, while carefully maintaining compatibility between the components and k8s.
- You must set up a CI/CD toolchain that not only expedites releases, but also ensures their quality, without additional efforts from your DevOps teams.

## #3 Providing enterprise-grade security in production.

## #4 Enabling in-depth and end-to-end governance:

- You need to set up an automated audit trail for your production deployments. 
- You must monitor infra elements like CPU, RAM etc. as well as abstractions like pods, replica sets etc.
- Version control for configurations, policies, containers and even infrastructure is crucial.
- You must have systems to generate reports for resource usage, utilization and saturation metrics to ensure cost management.

![image](https://user-images.githubusercontent.com/33947539/155497232-79b3e871-7e10-4ef7-a751-4767f95fbbd8.png)


## #5 Bringing consistency and visibility across multi-cloud environments:
Even though Kubernetes is consistent in providing environments, there can be differences across cloud vendors. Depending on how some of your services like load balancers and firewalls — which are not a native capability in open source k8s — are structured, your containers might work differently in different cloud environments.
DevOps teams must take care that these application services run effectively across multi-cloud deployments, which involves:

- Finding distribution that supports multi-cloud deployments. You must also make sure it accommodates the needs of each cloud platform.
- Ensuring you configure your cluster topology not just for the needs of your application, but also for multi-cloud environments.
- Minimizing inconsistencies among environments, often achieved through declarative approach, for smooth CI/CD.

## To summarize all the above:

#### 1. Perform Health Checks With Readiness and Liveness Probes
#### 2. Resource Management 

It is a good practice to specify resource requests and limits for individual containers. Another good practice is to divide Kubernetes environments into separate namespaces for different teams, departments, applications and clients.

![image](https://user-images.githubusercontent.com/33947539/155497898-c9370c1b-f322-4a7f-aaca-ee4ac173594c.png)

#### 3. Enable RBAC
![image](https://user-images.githubusercontent.com/33947539/155498090-4bf698f6-ff41-4d58-8e65-65ca6bb2f40c.png)

#### 4. Cluster Provisioning and Load Balancing

Production-grade Kubernetes infrastructure usually needs to have certain critical aspects such as high availability, multi-master, multi-etcd Kubernetes clusters, etc. The provisioning of such clusters typically involves tools such as Terraform or Ansible.

#### 5. Attach Labels to Kubernetes Objects
One important thing you can’t neglect is labels, as they allow Kubernetes objects to be queried and operated in bulk. The specialty of labels is that they can also be used to identify and organize Kubernetes objects into groups. One of the best use cases of this is grouping pods based on the application they belong to. 
 
#### 6. Set Network Policies

#### 7. Cluster Monitoring and Logging

#### 8. Enable the Use of Auto Scalers



