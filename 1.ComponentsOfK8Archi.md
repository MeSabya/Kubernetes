# Components of Kubernetes Architecture

- A Kubernetes cluster is made up of nodes. These nodes can be either physical machines, or virtual machines.
>Note: When you install Kubernetes on a System, you are actually installing the following components: an API Server, an ETCD service, a kubelet service, a Container Runtime, Controllers. and Schedulers.
## Kubernetes Components:
A Kubernetes cluster consists of nodes divided into two groups:
- A set of master nodes that host the Control Plane components, which are the brains of the system, since they control the entire cluster.
- A set of worker nodes that form the Workload Plane, which is where your workloads (or applications) run.
 

![image](https://user-images.githubusercontent.com/33947539/145934718-61c5e1d8-1109-41f8-9449-f5bcbc80025e.png)

![image](https://user-images.githubusercontent.com/33947539/145935167-1e3b50a1-f086-4396-8c42-960402cc336a.png)

## Kubectl
- kubectl is a command-line interface (CLI) for managing operations on Kubernetes clusters. 
- That’s it! It communicates with our good friend, the API Server, to get information about our cluster, and to tell Kubernetes to do stuff for us, like create a new resource, or modify an existing one. 

## Master Node
The master node is also known as a control plane that is responsible to manage worker/slave nodes efficiently. They interact with the worker node to

✔ Schedule the pods

✔ Monitor the worker nodes/Pods

✔ Start/restart the pods

✔ Manage the new worker nodes joining the cluster


As the manager of this whole operation, the master node has quite a few components:

    1. etcd
    2. API Server
    3. Controller Manager
    4. Scheduler

### etcd
- etcd is a distributed key-value database. Every time you make a change to Kubernetes — e.g. by way of sending it YAML or JSON via k8s’ REST API or via the kubectl CLI tool (more on that later) — that change is stored in etcd as JSON. 
- It is also versioned, so you also have some serious version control action going on.
- When it comes to Kubernetes, etcd reliably stores the configuration data of the Kubernetes cluster, 
  representing the state of the cluster (what nodes exist in the cluster, what pods should be running, which nodes they are running on, and a whole lot more) at any given point of time.
- As all cluster data is stored in etcd, you should always have a backup plan for it. You can easily back up your etcd data using the etcdctl snapshot save command. 
- In case you are running Kubernetes on AWS, you can also back up etcd by taking a snapshot of the EBS volume.
- ❗ Etcd also implements a watch feature, which provides an event-based interface for asynchronously monitoring changes to keys. Once a key is changed, its “watchers” get notified. This is a crucial feature in the context of Kubernetes, as the API Server component heavily relies on this to get notified and call the appropriate business logic components to move the current state towards the desired state.

### API Server
- Wanna talk to Kubernetes and tell it to do things for you? This is is your direct line — whether you’re a user, a program, or kubectl. 
- The API Server is also what’s responsible for sending data to and pulling data from etcd.The API Server is the only Kubernetes component that connects to etcd; all the other components must go through the API Server to work with the cluster state.
-  It is designed to scale horizontally — that is, it scales by deploying more instances.

### Kube-Controller-manager
This is a component on the master that runs controllers.
Logically, each controller is a separate process, but to reduce complexity, they are all compiled into a single binary and run in a single process.
These controllers include:

#### Node Controller: 
Responsible for noticing and responding when nodes go down.
#### Replication Controller: 
Responsible for maintaining the correct number of pods for every replication controller object in the system.
#### Endpoints Controller: 
Populates the Endpoints object (that is, it joins Services and Pods).
#### Service Account and Token Controllers: 
Create default accounts and API access tokens for new namespaces.

### Cloud-Controller-Manager
###### Cloud-controller-manager runs controllers that interact with the underlying cloud providers.
#### Node Controller: 
For checking the cloud provider to determine if a node has been deleted in the cloud after it stops responding.
#### Route Controller: 
For setting up routes in the underlying cloud infrastructure.
#### Service Controller: 
For creating, updating, and deleting cloud provider load balancers.
##### Volume Controller: 
For creating, attaching, and mounting volumes, and interacting with the cloud provider to orchestrate volumes.

### Scheduler
- The scheduler distributes work across multiple nodes. 
- Its looks at resource requirements (e.g. CPU and memory requirements) to figure out when to run a pod, and what node to run it on.

## Worker Node 

![image](https://user-images.githubusercontent.com/33947539/145937810-f145c55c-62c3-482c-99e1-a5fcd7511c12.png)

###### These run on every node, maintaining running pods and providing the Kubernetes runtime environment.

### Kubelet
- Its main job is to make sure that containers are running in a pod (wrapper of one or more containers). But who tells it to run these containers? That comes from the control plane (where our good friend the controller manager resides). When the control plane needs something to happen in a node, the kubelet makes it happen.
- The kubelet runs a container runtime, and, as its name implies, is responsible for actually running containers. More specifically, it manages the complete lifecycle of a container: container image pulling (from a container registry such as Docker Hub) and storage, container execution, network attachment, etc. Docker is a popular container runtime; however, there are others, such as containerd, CRI-O.
- Within a Kubernetes cluster, the kubelet watches for PodSpecs via the Kubernetes API server.
- A PodSpec is a YAML or JSON object that describes a pod. The kubelet takes a set of PodSpecs that are provided through various mechanisms (primarily through the API server) and ensures that the containers described in those PodSpecs are running and healthy.
- The Kubelet is the primary and most important controller in Kubernetes. It’s responsible for driving the container execution layer, typically Docker.

![image](https://user-images.githubusercontent.com/33947539/145938287-d6db675f-482c-4353-8aee-d4d76aad3c5f.png)

### Kube-proxy
- It runs in each node of your cluster and allows you to connect to pods from inside or outside of the cluster.
- The kube-proxy handles network communications inside and outside your cluster. This means that if pods need to talk to each other, or if some external service needs to talk to a pod, kube-proxy helps make that happen.

![image](https://user-images.githubusercontent.com/33947539/145938466-a7544fbd-3085-48ce-ad12-a5210e7b2a67.png)



 




 
