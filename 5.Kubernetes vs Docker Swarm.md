# Kubernetes vs Docker Swarm : Comparing Container Orchestration Tools

## Application definition & deployment

A Kubernetes deployment involves describing declarative updates to application states while updating Kubernetes Pods and ReplicaSets. By describing a Pod’s desired state, a controller changes the current state to the desired one at a regulated rate. With Kubernetes deployments, you can define all aspects of an application’s lifecycle. These aspects include:

- The number of pods
- Images to use
- How pods should be updated

In Docker Swarm, you deploy and define applications using predefined Swarm files to declare the desired state for the application. To deploy the app, you just need to copy the YAML file at the root level. This file, also known as the Docker Compose File, allows you to leverage its multiple node machine capabilities, thereby allowing organizations to run containers and services on:

- Multiple machines
- Any number of networks

## Availability

Kubernetes allows two topologies by default. These ensure high availability by creating clusters to eliminate single point of failures.

- You can use Stacked Control Plane nodes that ensure availability by co-locating etcd objects with all available nodes of a cluster during a failover.
- Or, you can use external etcd objects for load balancing, while controlling the control plane nodes separately.

Notably, both methods leverage using kubeadm and use a Multi-Master approach to maintain high availability, by maintaining etcd cluster nodes either externally or internally within a control plane.


![image](https://user-images.githubusercontent.com/33947539/153590669-e309482d-2690-42c4-9377-a99df4d68b9b.png)

Docker Swarm high availability is mainly based on the ability to clone services as Swarm nodes. The Swarm manager Nodes manage worker node resources and can move the node to another resource in case of failure.

## Scalability
Kubernetes supports autoscaling on both:

1. **The cluster level, through Cluster Autoscaling**
2. **The pod level, with Horizontal Pod Autoscaler**

Scaling in Kubernetes fundamentally involves creating new pods and scheduling it to nodes with available resources.

Docker Swarm deploys containers quicker. This gives the orchestration tool faster reaction times that allow for on-demand scaling. Scaling a Docker application to handle high traffic loads involves replicating the number of connections to the application. You can, therefore, easily scale your application up and down for even higher availability.

## Networking

Kubernetes creates a flat, peer-to-peer connection between pods and node agents for efficient inter-cluster networking. This connection includes network policies that regulate communication between pods while assigning distinct IP addresses to each of them. To define subnet, the Kubernetes networking model requires two Classless Inter-Domain Routers (CIDRs):

- One for Node IP Addressing
- The other for services

Docker Swarm creates two types of networks for every node that joins a Swarm:

- One network type outlines an overlay of all services within the network.
- The other creates a host-only bridge for all containers.

With a multi-layered overlay network, a peer-to-peer distribution among all hosts is achieved that enables secure and encrypted communications.

# Load balancing
- Kubernetes Pods can expose services externally, and Kubernetes performs load balancing for each service in the cluster, typically using an ingress.

- Docker Swarm uses DNS to distribute incoming requests to service names. It can automatically assign addresses to services, or the user can specify specific ports for the   
  service to run on.

# GUI

- Kubernetes provides dashboards via the Web UI that allow cluster administrators and developers to view and control cluster status.
- Docker Swarm does not offer a convenient built-in dashboard, and requires integration with open source or third-party tools.


