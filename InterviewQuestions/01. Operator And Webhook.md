## Kubernetes Operator (Kubebuilder, CRDs)

What is the role of a Kubernetes Operator? How is it different from a Helm chart?

Walk through the reconciliation loop â€” how does the controller-runtime reconcile CRs?

### How do you handle idempotency in your reconciler logic?
<details>
No matter how many times the reconciler runs for a resource, the resulting state should be the same â€” deterministic and safe to retry.

Reconciliation may happen many times due to:

- Object creation/update/delete
- Resyncs
- External events
- Watch triggers (secondary resource updates)
So, you must not create duplicates or perform unintended changes.

### Use deep equality checks before updates
```go
if !reflect.DeepEqual(found.Spec, desired.Spec) {
    found.Spec = desired.Spec
    err = r.Update(ctx, &found)
}
```
### Using GetGeneration 
```golang
// SetupWithManager sets up the controller with the Manager.
func (r *FooReconciler) SetupWithManager(mgr ctrl.Manager) error {
	return ctrl.NewControllerManagedBy(mgr).
		For(&tutorialv1.Foo{}).
		Watches(
			&source.Kind{Type: &corev1.Pod{}},
			handler.EnqueueRequestsFromMapFunc(r.mapPodsReqToFooReq),
		).
                WithEventFilter(predicate.Funcs{
			UpdateFunc: updateEventFilter,
		}).
		Complete(r)
}

// Custom function to filter update events.
func updateEventFilter(e event.UpdateEvent) bool {
        // Check if the generation has changed.
        // If yes, it means that the spec of the object was updated and thus we want to trigger a reconciliation loop.
       if (e.ObjectOld.GetGeneration() != e.ObjectNew.GetGeneration()) {
              return true;
       }

       // By default, do not trigger a reconciliation loop.
       return false;
}
```
#### What is .metadata.generation?
- generation is a number maintained by Kubernetes.
- It increments when the spec of an object changes.
- It does not increment on changes to status, metadata.labels, or annotations.

</details>


### How do you watch and react to changes in secondary resources (e.g., Pods owned by a CR)?

<details>

You've got a Foo custom resource:
```yaml
apiVersion: tutorial.example.com/v1
kind: Foo
metadata:
  name: my-foo
spec:
  name: my-app-pod
```

ðŸ‘‰ Let's say controllerâ€™s logic is:

- If a Pod with name my-app-pod exists â†’ set Foo.Status.happy = true
- If the Pod does not exist â†’ set Foo.Status.happy = false

***The Problem: CRs Are Not Notified When Other Resources Change
Letâ€™s say the Pod (my-app-pod) is deleted, restarted, or recreated.
Kubernetes will not automatically requeue Foo for reconciliation.***

Thatâ€™s a problem â€” your Foo.Status.happy will stay true forever, even though the Pod is gone ðŸ˜…
So here Foo is primary resource and pod created with the desired name is the secondary resource.

Without watching secondary resources:
- Your controller might operate on stale data
- Your CR's status will not reflect the true cluster state
- You lose reactiveness â€” things change, but your controller doesnâ€™t know

###  How It Works in Code
Hereâ€™s a standard pattern using controller-runtime:

```go
func (r *FooReconciler) SetupWithManager(mgr ctrl.Manager) error {
	return ctrl.NewControllerManagedBy(mgr).
		For(&tutorialv1.Foo{}).
		Watches(
			&source.Kind{Type: &corev1.Pod{}},
			handler.EnqueueRequestsFromMapFunc(r.mapPodsReqToFooReq),
		).
		Complete(r)
}
```

#### What Does EnqueueRequestsFromMapFunc Do?

It defines a mapping function. For each Pod event, this function decides:
Is this Pod relevant to any Foo?
If yes, enqueue a reconcile request for that Foo.

```go
func (r *FooReconciler) mapPodsReqToFooReq(ctx context.Context, pod client.Object) []reconcile.Request {
	var fooList tutorialv1.FooList
	_ = r.Client.List(ctx, &fooList)
	var reqs []reconcile.Request

	for _, foo := range fooList.Items {
		if foo.Spec.Name == pod.GetName() {
			reqs = append(reqs, reconcile.Request{
				NamespacedName: types.NamespacedName{Name: foo.Name, Namespace: foo.Namespace},
			})
		}
	}
	return reqs
}
```
This function gives you fine-grained control â€” you only trigger reconciliation for resources that actually care about the changed Pod.
 
</details>


## How does status subresource work in CRDs, and why is it important?

<details>
  
Great question â€” this comes up very frequently in interviews and real-world controller development.

Letâ€™s break it down clearly:

âœ… What is the status subresource in CRDs?
When you define a CRD, you can enable the status subresource, which separates the status field from the rest of the object.

Example:

```yaml
apiVersion: tutorial.example.com/v1
kind: Foo
metadata:
  name: my-foo
spec:
  replicas: 3
status:
  readyReplicas: 2
```

With the status subresource enabled, updates to .status are handled separately from .spec.

In your CRD YAML or Go types (Kubebuilder-style), you enable it like this:

```go
// +kubebuilder:subresource:status
type Foo struct {
    metav1.TypeMeta   `json:",inline"`
    metav1.ObjectMeta `json:"metadata,omitempty"`

    Spec   FooSpec   `json:"spec,omitempty"`
    Status FooStatus `json:"status,omitempty"`
}
```

This tells Kubernetes to create a separate /status endpoint (e.g., PATCH /apis/tutorial.example.com/v1/namespaces/default/foos/my-foo/status).

### Why is it important? 

#### Separation of concerns

- .spec is user-defined desired state
- .status is controller-updated actual state

Allowing users to update .status would be dangerous and misleading â€” status should reflect real system state.
With the subresource enabled, only the controller (with proper RBAC) can update .status.

#### Avoid race conditions
Two different controllers or processes can safely:

- One updates .spec
- The other updates .status
- Since they're hitting different API endpoints, this reduces the chances of accidental overwrites or update conflicts.

The status subresource in a CRD separates the .status field from the main resource. It ensures that only controllers can update the status, while users can safely modify the .spec. This separation avoids race conditions, enforces clear ownership of fields, and allows better validation. It also makes the controller pattern more robust by preventing accidental overwrites of important status information.
</details>

## when you do kubebuilder init and kubebuilder create api it registers your CRD types into the Scheme?

<details>

You need to manually call AddToScheme() only when you use types from outside your project (i.e., from other modules/packages) inside your controller or webhook.

### 1. Using other CRDs (from other projects)
Suppose your controller manages some external CRDs â€” for example:

Cert-Manager's Certificate

Istio's VirtualService

ArgoCD's Application

These types are not part of your own api/ directory.

âœ… So you have to import them and register their AddToScheme() too.

### 2. Custom Webhook Server
If you are writing a webhook server (admission/mutating/validating webhook),
the webhook server must decode incoming admission requests into your objects.

âœ… So you must ensure your CR types are in the scheme the webhook uses.

#### If your controller fetches, updates, or watches types that you did NOT define yourself, Add their AddToScheme() manually.

</details>

Explain finalizers in Kubernetes and how youâ€™ve used them in an operator.

What is the difference between OwnerReference and ControllerReference?

How do you design an Operator that syncs data across multiple clusters (subclouds)?

How do you handle CRD version upgrades and backward compatibility?

How do you manage errors in the reconciliation loop â€” retries, rate limiting, backoff?

## Webhooks (Validating/Mutating)
When would you use a Mutating Admission Webhook vs a Validating Admission Webhook?

How do you deploy and scale a webhook in a production cluster?

How do you ensure high availability and minimal latency in a webhook?

How do you secure webhooks using cert-manager or manual TLS?

How do you handle ordering and conflicts when multiple mutating webhooks are present?

Can a webhook cause a cluster outage? How would you prevent that?

How would you write a webhook to block deprecated API usage in the cluster?

How do you test a webhook locally before deploying?
