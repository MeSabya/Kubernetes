## Kubernetes Operator (Kubebuilder, CRDs)

What is the role of a Kubernetes Operator? How is it different from a Helm chart?

Walk through the reconciliation loop ‚Äî how does the controller-runtime reconcile CRs?

### How do you handle idempotency in your reconciler logic?
<details>
No matter how many times the reconciler runs for a resource, the resulting state should be the same ‚Äî deterministic and safe to retry.

Reconciliation may happen many times due to:

- Object creation/update/delete
- Resyncs
- External events
- Watch triggers (secondary resource updates)
So, you must not create duplicates or perform unintended changes.

### Use deep equality checks before updates
```go
if !reflect.DeepEqual(found.Spec, desired.Spec) {
    found.Spec = desired.Spec
    err = r.Update(ctx, &found)
}
```
### Using GetGeneration 
```golang
// SetupWithManager sets up the controller with the Manager.
func (r *FooReconciler) SetupWithManager(mgr ctrl.Manager) error {
	return ctrl.NewControllerManagedBy(mgr).
		For(&tutorialv1.Foo{}).
		Watches(
			&source.Kind{Type: &corev1.Pod{}},
			handler.EnqueueRequestsFromMapFunc(r.mapPodsReqToFooReq),
		).
                WithEventFilter(predicate.Funcs{
			UpdateFunc: updateEventFilter,
		}).
		Complete(r)
}

// Custom function to filter update events.
func updateEventFilter(e event.UpdateEvent) bool {
        // Check if the generation has changed.
        // If yes, it means that the spec of the object was updated and thus we want to trigger a reconciliation loop.
       if (e.ObjectOld.GetGeneration() != e.ObjectNew.GetGeneration()) {
              return true;
       }

       // By default, do not trigger a reconciliation loop.
       return false;
}
```
#### What is .metadata.generation?
- generation is a number maintained by Kubernetes.
- It increments when the spec of an object changes.
- It does not increment on changes to status, metadata.labels, or annotations.

</details>


### How do you watch and react to changes in secondary resources (e.g., Pods owned by a CR)?

<details>

You've got a Foo custom resource:
```yaml
apiVersion: tutorial.example.com/v1
kind: Foo
metadata:
  name: my-foo
spec:
  name: my-app-pod
```

üëâ Let's say controller‚Äôs logic is:

- If a Pod with name my-app-pod exists ‚Üí set Foo.Status.happy = true
- If the Pod does not exist ‚Üí set Foo.Status.happy = false

***The Problem: CRs Are Not Notified When Other Resources Change
Let‚Äôs say the Pod (my-app-pod) is deleted, restarted, or recreated.
Kubernetes will not automatically requeue Foo for reconciliation.***

That‚Äôs a problem ‚Äî your Foo.Status.happy will stay true forever, even though the Pod is gone üòÖ
So here Foo is primary resource and pod created with the desired name is the secondary resource.

Without watching secondary resources:
- Your controller might operate on stale data
- Your CR's status will not reflect the true cluster state
- You lose reactiveness ‚Äî things change, but your controller doesn‚Äôt know

###  How It Works in Code
Here‚Äôs a standard pattern using controller-runtime:

```go
func (r *FooReconciler) SetupWithManager(mgr ctrl.Manager) error {
	return ctrl.NewControllerManagedBy(mgr).
		For(&tutorialv1.Foo{}).
		Watches(
			&source.Kind{Type: &corev1.Pod{}},
			handler.EnqueueRequestsFromMapFunc(r.mapPodsReqToFooReq),
		).
		Complete(r)
}
```

#### What Does EnqueueRequestsFromMapFunc Do?

It defines a mapping function. For each Pod event, this function decides:
Is this Pod relevant to any Foo?
If yes, enqueue a reconcile request for that Foo.

```go
func (r *FooReconciler) mapPodsReqToFooReq(ctx context.Context, pod client.Object) []reconcile.Request {
	var fooList tutorialv1.FooList
	_ = r.Client.List(ctx, &fooList)
	var reqs []reconcile.Request

	for _, foo := range fooList.Items {
		if foo.Spec.Name == pod.GetName() {
			reqs = append(reqs, reconcile.Request{
				NamespacedName: types.NamespacedName{Name: foo.Name, Namespace: foo.Namespace},
			})
		}
	}
	return reqs
}
```
This function gives you fine-grained control ‚Äî you only trigger reconciliation for resources that actually care about the changed Pod.
 
</details>


## How does status subresource work in CRDs, and why is it important?

<details>
  
Great question ‚Äî this comes up very frequently in interviews and real-world controller development.

Let‚Äôs break it down clearly:

‚úÖ What is the status subresource in CRDs?
When you define a CRD, you can enable the status subresource, which separates the status field from the rest of the object.

Example:

```yaml
apiVersion: tutorial.example.com/v1
kind: Foo
metadata:
  name: my-foo
spec:
  replicas: 3
status:
  readyReplicas: 2
```

With the status subresource enabled, updates to .status are handled separately from .spec.

In your CRD YAML or Go types (Kubebuilder-style), you enable it like this:

```go
// +kubebuilder:subresource:status
type Foo struct {
    metav1.TypeMeta   `json:",inline"`
    metav1.ObjectMeta `json:"metadata,omitempty"`

    Spec   FooSpec   `json:"spec,omitempty"`
    Status FooStatus `json:"status,omitempty"`
}
```

This tells Kubernetes to create a separate /status endpoint (e.g., PATCH /apis/tutorial.example.com/v1/namespaces/default/foos/my-foo/status).

### Why is it important? 

#### Separation of concerns

- .spec is user-defined desired state
- .status is controller-updated actual state

Allowing users to update .status would be dangerous and misleading ‚Äî status should reflect real system state.
With the subresource enabled, only the controller (with proper RBAC) can update .status.

#### Avoid race conditions
Two different controllers or processes can safely:

- One updates .spec
- The other updates .status
- Since they're hitting different API endpoints, this reduces the chances of accidental overwrites or update conflicts.

The status subresource in a CRD separates the .status field from the main resource. It ensures that only controllers can update the status, while users can safely modify the .spec. This separation avoids race conditions, enforces clear ownership of fields, and allows better validation. It also makes the controller pattern more robust by preventing accidental overwrites of important status information.
</details>

## when you do kubebuilder init and kubebuilder create api it registers your CRD types into the Scheme?

<details>

You need to manually call AddToScheme() only when you use types from outside your project (i.e., from other modules/packages) inside your controller or webhook.

### 1. Using other CRDs (from other projects)
Suppose your controller manages some external CRDs ‚Äî for example:

Cert-Manager's Certificate

Istio's VirtualService

ArgoCD's Application

These types are not part of your own api/ directory.

‚úÖ So you have to import them and register their AddToScheme() too.

### 2. Custom Webhook Server
If you are writing a webhook server (admission/mutating/validating webhook),
the webhook server must decode incoming admission requests into your objects.

‚úÖ So you must ensure your CR types are in the scheme the webhook uses.

#### If your controller fetches, updates, or watches types that you did NOT define yourself, Add their AddToScheme() manually.

</details>

Explain finalizers in Kubernetes and how you‚Äôve used them in an operator.

What is the difference between OwnerReference and ControllerReference?

How do you design an Operator that syncs data across multiple clusters (subclouds)?

How do you handle CRD version upgrades and backward compatibility?

How do you manage errors in the reconciliation loop ‚Äî retries, rate limiting, backoff?

## Webhooks (Validating/Mutating)
When would you use a Mutating Admission Webhook vs a Validating Admission Webhook?

How do you deploy and scale a webhook in a production cluster?

How do you ensure high availability and minimal latency in a webhook?

## How do you secure webhooks using cert-manager or manual TLS?
Goal
We'll build a system where:

- cert-manager automatically issues a TLS certificate for your webhook service.
- The webhook pod uses the certificate.
- Kubernetes API server verifies the webhook's identity using the CA.
- No manual cert generation or renewal is needed.

<details>

### Directory structure

```bash
webhook-example/
‚îú‚îÄ‚îÄ deploy/
‚îÇ   ‚îú‚îÄ‚îÄ certificate.yaml
‚îÇ   ‚îú‚îÄ‚îÄ issuer.yaml
‚îÇ   ‚îú‚îÄ‚îÄ webhook-service.yaml
‚îÇ   ‚îú‚îÄ‚îÄ webhook-deployment.yaml
‚îÇ   ‚îú‚îÄ‚îÄ webhook-configuration.yaml
```

### Step 1: Define an Issuer
```yaml
# deploy/issuer.yaml
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: webhook-selfsigned-issuer
  namespace: default
spec:
  selfSigned: {}
```
This tells cert-manager to create a self-signed root CA (can be replaced with real CA for production).

### Step 2: Issue Certificate for Webhook Service

```yaml
# deploy/certificate.yaml
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: webhook-cert
  namespace: default
spec:
  dnsNames:
    - webhook.default.svc
  secretName: webhook-tls
  issuerRef:
    name: webhook-selfsigned-issuer
    kind: Issuer
  commonName: webhook.default.svc
```

This will generate:

- A TLS cert for webhook.default.svc
- A secret named webhook-tls with the cert and key

### Step 3: Webhook Service

```yaml
# deploy/webhook-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: webhook
  namespace: default
spec:
  selector:
    app: webhook
  ports:
    - port: 443
      targetPort: 8443
```

### Step 4: Webhook Deployment

```yaml
# deploy/webhook-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webhook
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webhook
  template:
    metadata:
      labels:
        app: webhook
    spec:
      containers:
        - name: webhook
          image: your-image:latest
          ports:
            - containerPort: 8443
          volumeMounts:
            - name: tls
              mountPath: /tls
              readOnly: true
          args:
            - "--tls-cert-file=/tls/tls.crt"
            - "--tls-private-key-file=/tls/tls.key"
      volumes:
        - name: tls
          secret:
            secretName: webhook-tls
```

### Step 5: Register Webhook with Kubernetes
```yaml
# deploy/webhook-configuration.yaml
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  name: example-mutating-webhook
webhooks:
  - name: webhook.default.svc
    clientConfig:
      service:
        name: webhook
        namespace: default
        path: /mutate
        port: 443
      caBundle: <INJECTED_AUTOMATICALLY_BY_CERT-MANAGER>
    rules:
      - operations: ["CREATE"]
        apiGroups: [""]
        apiVersions: ["v1"]
        resources: ["pods"]
    admissionReviewVersions: ["v1"]
    sideEffects: None
```

üîê caBundle can be injected manually or automated using cert-manager‚Äôs webhook injector or Helm charts.

### Final Steps
Apply all YAML files:

```bash
kubectl apply -f deploy/
```
Confirm that:

- cert-manager creates the secret webhook-tls
- Your deployment is using the mounted cert
- Webhook works correctly (e.g., mutates pod on creation)

![image](https://github.com/user-attachments/assets/25012434-0c90-424f-a32a-40e3f2ffce03)

</details>


How do you handle ordering and conflicts when multiple mutating webhooks are present?

Can a webhook cause a cluster outage? How would you prevent that?

How would you write a webhook to block deprecated API usage in the cluster?

How do you test a webhook locally before deploying?
