## CNI Defination and explanation 
CNI defines how the plugin should be developed and how container run times should invoke them.
CNI defines a set of responsibilities for container run times and plugins.

### Responsibilities For container run times:
- CNI specifies that it is responsible for creating a network namespace for each container.
- It should then identify the networks the container must attach to,
- container run time must then invoke the plugin when a container is created using the add command
- and also invoke the plugin when the container is deleted using the del command.
- It also specifies how to configure a network plugin on the container runtime environment using a JSON file.

### Responsibilities For plugin
- It defines that the plugin should support add, del and check command line arguments.
  and that these should accept parameters like container and network namespace.
- The plug-in should take care of assigning IP addresses to the pods
  and any associated routes required for the containers to reach other containers in the network.
- At the end, the results should be specified in a particular format.


## Container networking model (CNM).
Docker does not implement CNI. Docker has its own set of standards known as CNM which stands for container network model
which is another standard that aims at solving container networking challenges similar to CNI but with some differences.

Due to the differences,these plugins don't natively integrate with Docker,
meaning you can't run a Docker container and specify the network plugin to use a CNI and specify one of these plugins.

But that doesn't mean you can't use Docker with CNI at all.

### If docker does not have CNI then how docker containers works in k8s ?
Kubernetes does not directly manage containers; instead, it uses container runtimes to handle container operations. 
While Docker itself does not implement CNI, Kubernetes uses CNI plugins to handle the networking aspects for Pods. Docker containers work within Kubernetes through the integration provided by the CRI (via dockershim or other CRI-compliant runtimes like containerd) and the CNI plugin system, which ensures proper network setup and connectivity for containers running in the cluster. 

![image](https://github.com/MeSabya/Kubernetes/assets/33947539/c8ada07e-d326-4847-aed0-e6dac49c7921)


CNI is the standard way to provide networking to pods.

The main purpose of CNI is to allow different networking plugins to be used with container runtimes. This allows Kubernetes to be flexible and work with different networking solutions, such as Calico, Flannel, and Weave Net. CNI plugins are responsible for configuring network interfaces in pods, 
such as setting IP addresses, configuring routing, and managing network security policies.

![image](https://github.com/MeSabya/Kubernetes/assets/33947539/5f9e2e06-8bf5-4e3b-bb75-e4a8f3312b03)

- In Kubernetes, the kubelet is responsible for setting up the network for a new Pod using the CNI plugin specified
- in the network configuration file located in the /etc/cni/net.d/ directory on the node.
- This configuration file contains necessary parameters to configure the network for the Pod.
- When a pod is created, the kubelet reads the network configuration file and identifies the CNI plugin specified in the file.
- The kubelet then loads the CNI plugin and invokes its “ADD” command with the Pod’s network configuration parameters.
- The CNI plugin takes over and creates a network namespace, configures the network interface, and sets up routing and firewall
- rules based on the configuration parameters provided by the kubelet.
- The kubelet saves the actual network configuration parameters used by the CNI plugin in a file in the Pod’s network namespace,
- located in the /var/run/netns/ directory on the node.
- Finally, the kubelet notifies the container runtime, such as Docker, that the network is ready for the Pod to start.

## opt/cni/bin vs /etc/cni/net.d ..what these directories contain ?

The directories /opt/cni/bin and /etc/cni/net.d are related to Container Network Interface (CNI), 
a standard for configuring network interfaces in Linux containers. Here's what each directory typically contains:

### /opt/cni/bin:

This directory usually contains executable binaries for various CNI plugins.
CNI plugins are responsible for tasks such as network configuration, IP address management, routing, and connectivity between containers and the external network.
Each CNI plugin is a standalone binary that interacts with the container runtime (e.g., Docker, containerd, Kubernetes) to configure network settings for containers.
Examples of CNI plugins that may reside in this directory include bridge, host-local, flannel, calico, weave, etc.

### /etc/cni/net.d:

This directory typically contains configuration files (in JSON or other formats) for CNI plugins.
Each configuration file specifies how a particular CNI plugin should be invoked and configured.
Configuration files in this directory define network settings such as IP allocation ranges, network interfaces, bridges, routes, DNS configuration, and other parameters required for container networking.
Multiple configuration files can be present in this directory, allowing you to define different network configurations or use multiple CNI plugins simultaneously.

### In summary, 
/opt/cni/bin holds the executable binaries for CNI plugins, while /etc/cni/net.d contains the configuration files that specify 
how these plugins should be used and configured for container networking within the Kubernetes or container runtime environment. 
These directories are essential for setting up and managing container networking in a Kubernetes cluster or any containerized environment.


