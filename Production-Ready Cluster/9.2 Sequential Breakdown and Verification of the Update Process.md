Sequential Breakdown of the Update Process
Letâ€™s see what happened when we executed the kops update command.

Kops retrieved the desired state from the S3 bucket.

Kops sent requests to AWS API to change the values of the workers ASG.

AWS modified the values of the workers ASG by increasing them by 1.

ASG created a new EC2 instance to comply with the new sizing.

Protokube installed Kubelet and Docker and created the manifest file with the list of Pods.

Kubelet read the manifest file and run the container that forms the kube-proxy Pod (the only Pod on the worker nodes).

Kubelet sent a request to the kube-apiserver (through the dns-controller) to register the new node and join it to the cluster. The information about the new node is stored in etcd.

This process is almost identical to the one used to create the nodes of the cluster.

![image](https://user-images.githubusercontent.com/33947539/185801018-03f830a6-248c-48c9-8202-03b4f848b69f.png)
![image](https://user-images.githubusercontent.com/33947539/185801050-7856b803-01ae-4dcc-a404-8022e1befb5b.png)

Verification of the Update Process
Unless you are a very fast reader, ASG already created a new EC2 instance, and Kubelet joined it to the cluster. We can confirm that through the kops validate command.

![image](https://user-images.githubusercontent.com/33947539/185801073-cd6c5ebc-10da-41ec-a642-f0709e24aa8e.png)

We can see that now we have two nodes (there was one before) and that they are located somewhere inside the three us-east-2 availability zones.

Similarly, we can use kubectl to confirm that Kubernetes indeed added the new worker node to the cluster.

![image](https://user-images.githubusercontent.com/33947539/185801094-efb7d23b-0bba-401f-8dcf-60addaba5fde.png)
